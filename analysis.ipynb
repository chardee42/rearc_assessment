{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c7f398-57c3-4f74-abe2-9f67005e3acf",
   "metadata": {},
   "source": [
    "# Part 3: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f4c1d-8269-41db-be31-9bb7554b5a38",
   "metadata": {},
   "source": [
    "## Analysis Section 0\n",
    "### Load the CSV file\n",
    "pr.data.0.Current is tab delimited. Because of this, I added \"sep = '\\t'\" to handle the tabs.\n",
    "I also added code to remove leading and trailing spaces for the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8542f302-dc89-4d4e-a9a7-f4d50aaec956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+-----+--------------+\n",
      "|  series_id|year|period|value|footnote_codes|\n",
      "+-----------+----+------+-----+--------------+\n",
      "|PRS30006011|1995|   Q01|  2.6|          null|\n",
      "|PRS30006011|1995|   Q02|  2.1|          null|\n",
      "|PRS30006011|1995|   Q03|  0.9|          null|\n",
      "|PRS30006011|1995|   Q04|  0.1|          null|\n",
      "|PRS30006011|1995|   Q05|  1.4|          null|\n",
      "+-----------+----+------+-----+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim, col\n",
    "\n",
    "# Load the CSV file and view it\n",
    "df_csv = spark.read.csv(\"s3://rearc-chardee-project/bls-data/pr.data.0.Current\", header=True, sep = '\\t', inferSchema=True)\n",
    "\n",
    "# There are extra spaces (leading and trailing) in the column names, so I will use the line below to remove them.\n",
    "df_csv = df_csv.toDF(*[c.strip() for c in df_csv.columns])\n",
    "\n",
    "\n",
    "# Automatically trim all string columns in a DataFrame\n",
    "for c in df_csv.columns:\n",
    "    if dict(df_csv.dtypes)[c] == 'string':\n",
    "        df_csv = df_csv.withColumn(c, trim(col(c)))\n",
    "df_csv.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c340907-5fd4-4e86-8a84-b4fe53eb60ff",
   "metadata": {},
   "source": [
    "### Load the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "28ac125e-428a-45c5-94c2-c21bc9a4580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- annotations: struct (nullable = true)\n",
      " |    |-- dataset_link: string (nullable = true)\n",
      " |    |-- dataset_name: string (nullable = true)\n",
      " |    |-- source_description: string (nullable = true)\n",
      " |    |-- source_name: string (nullable = true)\n",
      " |    |-- subtopic: string (nullable = true)\n",
      " |    |-- table_id: string (nullable = true)\n",
      " |    |-- topic: string (nullable = true)\n",
      " |-- columns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Nation: string (nullable = true)\n",
      " |    |    |-- Nation ID: string (nullable = true)\n",
      " |    |    |-- Population: double (nullable = true)\n",
      " |    |    |-- Year: long (nullable = true)\n",
      " |-- page: struct (nullable = true)\n",
      " |    |-- limit: long (nullable = true)\n",
      " |    |-- offset: long (nullable = true)\n",
      " |    |-- total: long (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file.\n",
    "# If you load the JSON, it will give you a corrupt record error. \n",
    "# I added \"multiLine=True\" to handle this.\n",
    "\n",
    "df_json = spark.read.json(\"s3://rearc-chardee-project/api_out.json\", multiLine=True)\n",
    "df_json.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a28b32fb-51ce-4a97-941d-8879fd99a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|         annotations|             columns|                data|      page|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|{http://www.censu...|[Nation ID, Natio...|[{United States, ...|{0, 0, 10}|\n",
      "+--------------------+--------------------+--------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "# As you can see from the output, it needs more work. \n",
    "# It is only showing one record.\n",
    "\n",
    "df_json.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87f11f-b970-420e-b106-eb317c73f21a",
   "metadata": {},
   "source": [
    "### Explode the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bcb5d192-6397-4573-a6bd-4619741beb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|row                                         |\n",
      "+--------------------------------------------+\n",
      "|{United States, 01000US, 3.16128839E8, 2013}|\n",
      "|{United States, 01000US, 3.18857056E8, 2014}|\n",
      "|{United States, 01000US, 3.21418821E8, 2015}|\n",
      "|{United States, 01000US, 3.23127515E8, 2016}|\n",
      "|{United States, 01000US, 3.25719178E8, 2017}|\n",
      "+--------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# To turn this into multiple records, we will need to explode the data.\n",
    "# Explode and view the data\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_json_explode = df_json.select(explode(\"data\").alias(\"row\"))\n",
    "df_json_explode.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aff064-13c5-463a-8f1d-cc65ed41512a",
   "metadata": {},
   "source": [
    "### Flatten the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2dea679f-f8e5-425a-a8dc-c1b682b89fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----+------------+\n",
      "|Nation_ID|       Nation|Year|  Population|\n",
      "+---------+-------------+----+------------+\n",
      "|  01000US|United States|2013|3.16128839E8|\n",
      "|  01000US|United States|2014|3.18857056E8|\n",
      "|  01000US|United States|2015|3.21418821E8|\n",
      "|  01000US|United States|2016|3.23127515E8|\n",
      "|  01000US|United States|2017|3.25719178E8|\n",
      "|  01000US|United States|2018|3.27167439E8|\n",
      "|  01000US|United States|2019|3.28239523E8|\n",
      "|  01000US|United States|2021|3.31893745E8|\n",
      "|  01000US|United States|2022|3.33287562E8|\n",
      "|  01000US|United States|2023|3.34914896E8|\n",
      "+---------+-------------+----+------------+\n"
     ]
    }
   ],
   "source": [
    "# This is closer, but all of the data is still is one column. It will need to be flattened next.\n",
    "df_json_flat = df_json_explode.select(\n",
    "    df_json_explode.row[\"Nation ID\"].alias(\"Nation_ID\"),\n",
    "    df_json_explode.row[\"Nation\"].alias(\"Nation\"),\n",
    "    df_json_explode.row[\"Year\"].alias(\"Year\"),\n",
    "    df_json_explode.row[\"Population\"].alias(\"Population\")\n",
    ")\n",
    "\n",
    "df_json_flat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4526b-c374-43a1-9e44-c242dff08397",
   "metadata": {},
   "source": [
    "## Analysis Section 1\n",
    "### Filter the data from 2013 to 2018 inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9a131398-1c3f-4c7f-8428-ab6b17e3c57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----+------------+\n",
      "|Nation_ID|       Nation|Year|  Population|\n",
      "+---------+-------------+----+------------+\n",
      "|  01000US|United States|2013|3.16128839E8|\n",
      "|  01000US|United States|2014|3.18857056E8|\n",
      "|  01000US|United States|2015|3.21418821E8|\n",
      "|  01000US|United States|2016|3.23127515E8|\n",
      "|  01000US|United States|2017|3.25719178E8|\n",
      "|  01000US|United States|2018|3.27167439E8|\n",
      "+---------+-------------+----+------------+\n"
     ]
    }
   ],
   "source": [
    "# Now the data is in a format I can use.\n",
    "\n",
    "# Let's recap with the instructions:\n",
    "\n",
    "## Using the dataframe from the population data API (Part 2), generate the mean and the standard deviation\n",
    "## of the annual US population across the years [2013, 2018] inclusive.\n",
    "\n",
    "df_json_filter = df_json_flat.filter(\n",
    "    (df_json_flat['Year'] >= 2013) & (df_json_flat['Year'] <= 2018)\n",
    ")\n",
    "\n",
    "df_json_filter.show()\n",
    "\n",
    "# Now we have the date range we need (2013 to 2018 inclusive) in df_json_filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad95c5-8bc4-4789-b3f6-49cd74fb1c50",
   "metadata": {},
   "source": [
    "### Calculate the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "298ade9d-7dd4-417d-90bc-aee4f0f9a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322069808.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean of the Population column\n",
    "from pyspark.sql import functions as F\n",
    "df_json_filter.agg(F.mean('Population')).collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3405a70-e250-466c-8c8a-07df8af9b4a1",
   "metadata": {},
   "source": [
    "### Calculate the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "577141a2-b3f8-4d1e-8fa5-4be7ca0bf393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4158441.040908092\n"
     ]
    }
   ],
   "source": [
    "# Calculate the standard deviation of the column.\n",
    "stddev_value = df_json_filter.agg(F.stddev(\"Population\")).collect()[0][0]\n",
    "print(stddev_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf1ceb-3155-4bc5-8b68-28e0364fc27a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae6351-e7e9-40ba-a4f9-a3c75c402a3a",
   "metadata": {},
   "source": [
    "## Analysis Section #2\n",
    "\n",
    "Here are the instructions:\n",
    "\n",
    "Using the dataframe from the time-series (Part 1), For every series_id, find the best year: the year with the max/largest sum of \"value\" for all quarters in that year. Generate a report with each series id, the best year for that series, and the summed value for that year.\n",
    "\n",
    "This data is already loaded into df_csv at the top of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e8403646-8893-41f0-8220-b0a8626323f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+-----+--------------+\n",
      "|  series_id|year|period|value|footnote_codes|\n",
      "+-----------+----+------+-----+--------------+\n",
      "|PRS30006011|1995|   Q01|  2.6|          null|\n",
      "|PRS30006011|1995|   Q02|  2.1|          null|\n",
      "|PRS30006011|1995|   Q03|  0.9|          null|\n",
      "|PRS30006011|1995|   Q04|  0.1|          null|\n",
      "|PRS30006011|1995|   Q05|  1.4|          null|\n",
      "+-----------+----+------+-----+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "577e3c24-676a-465c-8f32-f3a7baf1d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+-----+\n",
      "|        series_id|year|value|\n",
      "+-----------------+----+-----+\n",
      "|PRS30006012      |2022| 17.1|\n",
      "|PRS30006023      |2014|503.2|\n",
      "|PRS31006093      |2014|507.3|\n",
      "|PRS31006131      |2021| 12.7|\n",
      "|PRS32006102      |2003| 34.6|\n",
      "|PRS32006132      |2021| 21.3|\n",
      "|PRS32006232      |2021| 22.6|\n",
      "|PRS84006151      |2020| 33.2|\n",
      "|PRS85006152      |2020| 42.7|\n",
      "|PRS85006162      |2020| 26.3|\n",
      "+-----------------+----+-----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a window partitioned by series_id. \n",
    "# Order by Total_Value\n",
    "wind = Window.partitionBy(\"series_id\").orderBy(F.col(\"Total_Value\").desc())\n",
    "\n",
    "# Create a new column called \"ranker\"\n",
    "df_ranked = df_csv_grp.withColumn(\"ranker\", F.rank().over(wind))\n",
    "\n",
    "# Show only the series and years with a number 1 ranking (top)\n",
    "# Then select the correct columns for the output.\n",
    "df_output1 = df_ranked.filter(F.col(\"ranker\") == 1) \\\n",
    "                          .select(\"series_id\", \"year\", F.col(\"Total_Value\").alias(\"value\"))   \n",
    "                          \n",
    "# Clean up the values to 1 decimal place.\n",
    "df_output2 = df_output1.withColumn(\"value\", F.round(F.col(\"value\"), 1))\n",
    "df_output2.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424e77e-183b-4d52-9849-f27a18b51b32",
   "metadata": {},
   "source": [
    "***\n",
    "## Analysis Section 3\n",
    "Using both dataframes from Part 1 and Part 2, generate a report that will provide the value for series_id = PRS30006032 and period = Q01 and the population for that given year (if available in the population dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "aa2cb1a7-c373-48b6-bba5-38222e1e3a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+-----+----------+\n",
      "|  series_id|year|period|value|Population|\n",
      "+-----------+----+------+-----+----------+\n",
      "|PRS30006032|2018|   Q01|  0.5| 327167439|\n",
      "+-----------+----+------+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "# For this I will use df_csv and df_json_flat.\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_filter_prs = df_csv.filter(\n",
    "    (F.col(\"series_id\") == \"PRS30006032\") &\n",
    "    (F.col(\"period\") == \"Q01\") &\n",
    "    (F.col(\"year\") == \"2018\")\n",
    ")\n",
    "\n",
    "df_json_2018 = df_json_flat.filter(F.col(\"Year\") == '2018')\n",
    "\n",
    "df_json_20182 = df_json_2018.withColumn(\"Population2\", col(\"Population\").cast(\"double\").cast(\"long\"))\n",
    "\n",
    "# df1.join(df2, on=key, how=how)\n",
    "df_joined_2018 = df_filter_prs.join(\n",
    "    df_json_20182,\n",
    "    df_filter_prs['year'] == df_json_20182['Year'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# df_joined_2018.show()\n",
    "\n",
    "# df_filter_prs.show(5)\n",
    "# df_json_20182.show(5)\n",
    "\n",
    "# df_joined_2018[['series_id', 'year', 'period', 'value', 'Population']]\n",
    "df_join_final = df_joined_2018.select(\"series_id\", df_json_2018[\"year\"].alias(\"year_from_left\"),  \"period\", \"value\", \"Population2\")\n",
    "\n",
    "# Fix the column names so they match\n",
    "df_join_final = df_join_final.toDF(\"series_id\", \"year\", \"period\", \"value\", \"Population\")\n",
    "\n",
    "df_join_final.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab216b-baeb-493f-84d1-1eb3fa09a125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
